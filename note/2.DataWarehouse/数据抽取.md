## 数仓-ETL
#### 1. Extract
##### 数据抽取 
     数据源可以分为结构化数据,非结构化数据,半结构化数据
     结构化数据一般采用JDBC(主从库)或者数据库binlog日志,半/非结构化数据会采用实时或者监听文件变动的方式(比如oracle ogg方式)
##### 抽取方式
      数据抽取的方式可以分为: 增量抽取 以及 全量抽取
      全量同步一般用于初始化数据,当然也可每日全量同步数据
      增量同步一般数据行的某个时间或者某个标志进行数据更新,改方式同步成本小,前提必须要与数据源系统保持更新标志的一致性
#### 2.Transform 
     数据转换分为清洗和转换两个
     数据清洗主要是对数据的重复,二义性,不完整性,不符合业务逻辑的数据进行处理或者过滤
     数据转换是对数据进行标准化处理,比如时间戳转日期格式,json解析等等

#### 3.Load
     数据加载进入目标表
    
#### 4.常用ETL工具
##### 1.Sqoop
###### 1. 定义:SQOOP 用于关系型数据库与hadoop平台之间的数据传输工具(import/export)
###### 2.工作机制: 将import/export 命令翻译成MR任务后执行
###### 3.Sqoop1 与Sqoop2区别
###### 4.语法: 
       sqoop  import \
       --connect jdbc:mysql://192.168.2.xx:3306/xxx  \  ## 数据库url以及库名
       --username root \    ## 关系型数据库用户名
       --password xxx \     ## 关系型数据库用户密码
       --table xxx  \       ## 关系型数据库标明
       --hive-database xxx  \   ## hive 库名
       --hive-table xxx  \      ## hive 表明
       --hive-import \       
       --hive-overwrite  \      ## 覆盖掉在Hive表中已经存在的数据
       --null-string '\\N' \    ## string 类型的列如果为null时,则置为\\N
       --null-non-string '\\N' \ ## 非string 类型的列如果为null时,则置为\\N
       -m 1                      \ ## 启动N个map来并行导入数据,默认为4
       --target-dir /xxx/xxx  \　　　　　　# HDFS 的目标存储位置
       --where "id = 1000" \　　　　　　　　　# 指定条件，关系型数据库的where条件
       --columns "c1,c2" \　　# 指定需要的字段
       --check-column id \    # 增量字段
       --incremental lastmodified。你可以使用 \  # Sqoop支持两种增量导入：append和 lastmodified。你可以使用--incremental 参数来决定使用使用那种类型。当一个表持续的使用自增ID来增加数据时，应该指定append模式，使用--check-column来制定包含行ID的字段。
       --last-value 3        \ #--last-value设置一个值，sqoop在导入数据时会选择比这个值大的数据进行导入。

       sqoop import 进多分区表的方法:（直接加在到分区目录或者修改源码）

###### 5.参数(https://www.cnblogs.com/alexzhang92/p/10927884.html) import:
|      | 参数  | 说明  |
|  ----  | ---- | ----  |
|   | --enclosed-by <char> | 给字段值前加上指定的字符 |
|   | --escaped-by <char> | 对字段中的双引号加转义符 |
|   | --fields-terminated-by <char> | 设定每个字段是以什么符号作为结束，默认为逗号 |
|   | --lines-terminated-by <char> | 设定每行记录之间的分隔符，默认是\n |
|   | --mysql-delimiters | Mysql默认的分隔符设置，字段之间以逗号分隔，行之间以\n分隔，默认转义符是\，字段值以单引号包裹。 |
|   | --optionally-enclosed-by <char> | 给带有双引号或单引号的字段值前后加上指定字符。 |
|   | --incremental <mode> | 指定sqoop如何选定新行。包括append、lastmodified两种模式（append不能与--hive-等参数同时使用） |
|   | --hive-delims-replacement <arg> | 用自定义的字符串替换掉数据中的\r\n和\013 \010等字符 |
|   | --hive-drop-import-delims | 在导入数据到hive时，去掉数据中的\r\n\013\010这样的字符 |
|   | --map-column-hive <arg> | 生成hive表时，可以更改生成字段的数据类型,--map-column-hive column_name = timestamp\ |
|   | --check-列 <char>| 作为增量导入判断的列名 |
|   | --hive-partition-key <KEY> | 创建分区，后面直接跟分区名，分区字段的默认类型为string,多分区的话可以直接数据加载到指定分区目录 |
|   | --hive-partition-value <v> | 导入数据时，指定某个分区的值 |
|   | --hive-overwrite | 覆盖掉在hive表中已经存在的数据 |
|   | --create-hive-table | 默认是false，即，如果目标表已经存在了，那么创建任务失败。 |
|   | --as-textfile | 将数据导入到一个普通文本文件中 |
|   | --query或--e <statement> | 将查询结果的数据导入，使用时必须伴随参--target-dir，--hive-table，如果查询中有where条件，则条件后必须加上$CONDITIONS关键 |
|   | --m或–num-mappers | 启动N个map来并行导入数据，默认4个 |
       
##### 2.FLUME
      Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力